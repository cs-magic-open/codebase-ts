// This is your Prisma schema file,
// learn more about it in the docs: https://pris.ly/d/prisma-schema

generator client {
  provider = "prisma-client-js"
}

generator json {
  /// Always after the prisma-client-js generator
  provider  = "npx prisma-json-types-generator"
  namespace = "PrismaJson"
  // clientOutput = "<finds it automatically>"
  // (./ -> relative to schema, or an importable path to require() it)
  // useType = "MyType"
  // In case you need to use a type, export it inside the namespace and we will add a index signature to it
  // (e.g.  export namespace PrismaJson { export type MyType = {a: 1, b: 2} }; will generate namespace.MyType["TYPE HERE"])
}

datasource db {
  provider = "postgresql"
  // NOTE: When using mysql or sqlserver, uncomment the @db.Text annotations in model Account below
  // Further reading:
  // https://next-auth.js.org/adapters/prisma#create-the-prisma-schema
  // https://www.prisma.io/docs/reference/api-reference/prisma-schema-reference#string
  url      = env("DATABASE_URL")
}

// Necessary for Next auth
model Account {
  id                String  @id @default(nanoid(5))
  userId            String
  type              String
  provider          String
  providerAccountId String
  refresh_token     String? // @db.Text
  access_token      String? // @db.Text
  expires_at        Int?
  token_type        String?
  scope             String?
  id_token          String? // @db.Text
  session_state     String?
  user              User    @relation(fields: [userId], references: [id], onDelete: Cascade)

  @@unique([provider, providerAccountId])
}

model App {
  id        String   @id @default(nanoid(5))
  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  fromUser User   @relation(fields: [user], references: [id], onDelete: Cascade)
  // string
  // Optional
  // A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. Learn more.
  user     String

  model     Model  @relation(fields: [modelName], references: [id], onDelete: Cascade)
  // ID of the model to use. See the model endpoint compatibility table for details on which models work with the Chat API.
  modelName String

  convs     Conv[]
  requests  Request[]
  responses Response[]

  systemPrompt String?
  title        String?

  // 授权上架后的才可以展示在选择框内
  granted Boolean @default(false)

  // -------
  // langchain的代码 OpenAIBaseInput
  // todo: nai doc: https://platform.openai.com/docs/api-reference/chat/create
  // -------

  // number or null
  // Optional
  // Defaults to 1
  // What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
  //
  // We generally recommend altering this or top_p but not both.
  temperature Float? @default(0.7)

  // integer or null
  // Optional
  // The maximum number of tokens that can be generated in the chat completion.
  //
  // The total length of input tokens and generated tokens is limited by the model's context length. Example Python code for counting tokens.
  maxTokens Int? @default(4096)

  // number or null
  // Optional
  // Defaults to 1
  // An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
  //
  // We generally recommend altering this or temperature but not both.
  topP Float? @default(0.5)

  // number or null
  // Optional
  // Defaults to 0
  // Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
  frequencyPenalty Float? @default(0)

  // number or null
  // Optional
  // Defaults to 0
  // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
  presencePenalty Float? @default(0)

  // integer or null
  // Optional
  // Defaults to 1
  // How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep n as 1 to minimize costs.
  n Int? @default(1)

  // map
  // Optional
  // Defaults to null
  // Modify the likelihood of specified tokens appearing in the completion.
  //
  // Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
  // logitBias?: Record<string, number>;

  // boolean or null
  // Optional
  // Defaults to false
  // If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. Example Python code.
  streaming Boolean? @default(true)

  // /** Holds any additional parameters that are valid to pass to {@link
  // * https://platform.openai.com/docs/api-reference/completions/create |
  // * `openai.createCompletion`} that are not explicitly specified on this class.
  // */
  // modelKwargs?: Record<string, any>;

  // string / array / null
  // Optional
  // Defaults to null
  // Up to 4 sequences where the API will stop generating further tokens.
  stop String[] @default([])

  // /**
  // * Timeout to use when making requests to OpenAI.
  // */
  timeout Int? @default(3000)

  // /**
  // * API key to use when making requests to OpenAI. Defaults to the value of
  // * `OPENAI_API_KEY` environment variable.
  //*/
  openAIApiKey String?
}

model Company {
  id    String  @id @default(nanoid(5))
  title String
  url   String?
  logo  String?

  models Model[]
}

model Conv {
  id        String   @id @default(nanoid(7))
  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  fromUser   User   @relation(fields: [fromUserId], references: [id], onDelete: Cascade)
  fromUserId String

  title    String?
  requests Request[]

  apps App[]
}

model Model {
  id        String   @id @default(nanoid(5))
  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  title String
  logo  String?
  url   String?

  company   Company @relation(fields: [companyId], references: [id], onDelete: Cascade)
  companyId String
  apps      App[]
}

model Request {
  id        String   @id @default(nanoid(9))
  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  /// [QueryContex]
  context Json

  conv   Conv   @relation(fields: [convId], references: [id], onDelete: Cascade)
  convId String

  apps App[]

  responses Response[]
}

model Response {
  id        String   @id @default(nanoid(9))
  createdAt DateTime @default(now())
  updatedAt DateTime @updatedAt

  request   Request @relation(fields: [requestId], references: [id], onDelete: Cascade)
  requestId String

  app   App    @relation(fields: [appId], references: [id], onDelete: Cascade)
  appId String

  response String?
  error    String?

  tTrigger DateTime?
  tStart   DateTime?
  tEnd     DateTime?
}

model Session {
  id           String   @id @default(nanoid(5))
  sessionToken String   @unique
  userId       String
  expires      DateTime
  user         User     @relation(fields: [userId], references: [id], onDelete: Cascade)
}

model User {
  id            String    @id @default(nanoid(5))
  name          String?
  email         String?   @unique
  emailVerified DateTime?
  image         String?
  accounts      Account[]
  sessions      Session[]

  queryConversations Conv[]
  queryConfigs       App[]
}

model VerificationToken {
  identifier String
  token      String   @unique
  expires    DateTime

  @@unique([identifier, token])
}
